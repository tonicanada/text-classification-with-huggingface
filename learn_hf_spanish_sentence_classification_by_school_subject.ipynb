{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef6ab608-e5ee-4f5c-bf1a-3bb77c92adcb",
   "metadata": {},
   "source": [
    "# Tutorial proyecto de clasificaci√≥n de texto mediante Hugging Face (multiclase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feab20f0-13c6-43ba-81eb-c34f77b9cbd9",
   "metadata": {},
   "source": [
    "## ¬øQu√© vamos a construir?\n",
    "\n",
    "Bienvenido al tutorial del proyecto de clasificaci√≥n de texto mediante Hugging Face. Este notebook est√° dise√±ado para que pueda ser reutilizable para otros casos.\n",
    "En concreto, vamos a construir un modelo, que a partir de una frase en espa√±ol, va a determinar de qu√© asignatura se trata, pudiendo ser una de las siguientes:\n",
    "* Religi√≥n\n",
    "* Lengua y literatura\n",
    "* Educaci√≥n f√≠sica\n",
    "* Artes\n",
    "* Idiomas extranjeros\n",
    "* Historia\n",
    "* Geograf√≠a\n",
    "* F√≠sica y qu√≠mica\n",
    "* Matem√°ticas\n",
    "* Frase no relacionada con asignaturas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4db81c8b-0fbe-4272-8a21-cf401af0564a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d022d7c-cdd0-4d07-9d21-fdbc6863e24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acm/Coding/ztm_courses/ztm-huggingface/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Uploading the dataset shards:   0%|                       | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 408.56ba/s]\u001b[A\n",
      "Uploading the dataset shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.25it/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/tonicanada/learn_hf_spanish_sentence_classification_by_school_subject/commit/daaf3830f889b6ac91ae95714d3464b58c7e3d38', commit_message='Upload dataset', commit_description='', oid='daaf3830f889b6ac91ae95714d3464b58c7e3d38', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/tonicanada/learn_hf_spanish_sentence_classification_by_school_subject', endpoint='https://huggingface.co', repo_type='dataset', repo_id='tonicanada/learn_hf_spanish_sentence_classification_by_school_subject'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pprint\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "# Subimos el dataset a HF datasets\n",
    "DATASET_NAME = \"tonicanada/learn_hf_spanish_sentence_classification_by_school_subject\"\n",
    "df = pd.read_excel(\"./datasets/spanish_sentence_classification_by_school_subject_dataset.xlsx\")\n",
    "dataset = datasets.Dataset.from_pandas(df)\n",
    "repo_id_dataset = DATASET_NAME\n",
    "dataset.push_to_hub(repo_id_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3433c45-9d21-448f-8fb9-9b73a7958fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Definimos las variables para el entrenaiento del modelo y pipeline\n",
    "MODEL_NAME = \"distilbert/distilbert-base-multilingual-cased\"\n",
    "MODEL_SAVE_DIR_NAME=\"models/learn_hf_spanish_sentence_classification_by_school_subject\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c4f5001-05b6-40aa-81ea-2f3464e45701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Descargando dataset desde Hugging Face Hub, nombre: tonicanada/learn_hf_spanish_sentence_classification_by_school_subject\n"
     ]
    }
   ],
   "source": [
    "# 2. Cargamos y preprocesamos el dataset desde Hugging Face Hub\n",
    "print(f\"[INFO] Descargando dataset desde Hugging Face Hub, nombre: {DATASET_NAME}\")\n",
    "dataset = datasets.load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "049de038-533d-4320-8e77-80c6ed47bddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Religi√≥n', 1: 'Frase no relacionada con asignaturas', 2: 'Lengua y literatura', 3: 'Educaci√≥n f√≠sica', 4: 'Artes', 5: 'Idiomas extranjeros', 6: 'Historia', 7: 'Geograf√≠a', 8: 'F√≠sica y qu√≠mica', 9: 'Matem√°ticas'}\n"
     ]
    }
   ],
   "source": [
    "# Creamos una funci√≥n que permita transformar las labels a id\n",
    "id2label = {idx: label for idx, label in enumerate(dataset['train'].unique('label')[::-1])}\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc4e9468-cd38-44e3-9e92-7802fe9e4b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Religi√≥n': 0,\n",
       " 'Frase no relacionada con asignaturas': 1,\n",
       " 'Lengua y literatura': 2,\n",
       " 'Educaci√≥n f√≠sica': 3,\n",
       " 'Artes': 4,\n",
       " 'Idiomas extranjeros': 5,\n",
       " 'Historia': 6,\n",
       " 'Geograf√≠a': 7,\n",
       " 'F√≠sica y qu√≠mica': 8,\n",
       " 'Matem√°ticas': 9}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {label: id for id, label in id2label.items()}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4261d22-df57-46e2-b3db-83dee80454d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una funci√≥n para mapear las labels a ID en el dataset\n",
    "def map_labels_to_number(example):\n",
    "    example[\"label\"] = label2id[example[\"label\"]]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a1c0225-f3c3-4af7-8fb8-6e21000c852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train'].map(map_labels_to_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f658c44e-49b6-48b4-850c-d43fe2a78e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos dataset en train/test sets\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5444e153-36ff-4c9e-b92c-9956bc4a8931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La Ant√°rtida es el continente m√°s fr√≠o</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El √°rea de un c√≠rculo se calcula con la f√≥rmul...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El desierto del Sahara se encuentra en el nort...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El ensayo es un texto que presenta las ideas d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>El voleibol se juega en una cancha dividida po...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0             La Ant√°rtida es el continente m√°s fr√≠o      7\n",
       "1  El √°rea de un c√≠rculo se calcula con la f√≥rmul...      9\n",
       "2  El desierto del Sahara se encuentra en el nort...      7\n",
       "3  El ensayo es un texto que presenta las ideas d...      2\n",
       "4  El voleibol se juega en una cancha dividida po...      3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset['train'].to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11b38423-54a5-46d5-b3e9-6b95af4cf303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Tokenizando text para entrenamiento de modelo: distilbert/distilbert-base-multilingual-cased\n"
     ]
    }
   ],
   "source": [
    "# Importamos un tokenizer y lo mapeamos en el dataset\n",
    "print(f\"[INFO] Tokenizando text para entrenamiento de modelo: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=MODEL_NAME,\n",
    "                                          use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa6834d9-fda4-4e9e-9177-951c9a2f185d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '[UNK]', '[SEP]']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hacemos pruebas de conversi√≥n de texto a n√∫meros con el tokenizer\n",
    "tokenizer(\"Ciencia\")\n",
    "tokenizer.convert_ids_to_tokens(tokenizer(\"üòÜ\").input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5666309-a419-4711-b2e4-3124999800bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos funci√≥n para tokenize la columna \"text\" del dataset\n",
    "def tokenize_text(examples):\n",
    "    \"\"\"\n",
    "    Tokeniza un texto.\n",
    "    \"\"\"\n",
    "    return tokenizer(examples['text'],\n",
    "                    padding=True,\n",
    "                    truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc1b6adc-cd9e-4d7a-8dc7-f9c0a8a66df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 23453.49 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 11630.81 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(function=tokenize_text,\n",
    "                                batched=True,\n",
    "                                batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6a77fed-f726-4baa-973b-2ce14a72f2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La Ant√°rtida es el continente m√°s fr√≠o</td>\n",
       "      <td>7</td>\n",
       "      <td>[101, 10159, 40328, 46532, 11726, 10196, 10125...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El √°rea de un c√≠rculo se calcula con la f√≥rmul...</td>\n",
       "      <td>9</td>\n",
       "      <td>[101, 10224, 13487, 10104, 10119, 78443, 10126...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El desierto del Sahara se encuentra en el nort...</td>\n",
       "      <td>7</td>\n",
       "      <td>[101, 10224, 10139, 93548, 10127, 38836, 10126...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El ensayo es un texto que presenta las ideas d...</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 10224, 55683, 50253, 10196, 10119, 27888...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>El voleibol se juega en una cancha dividida po...</td>\n",
       "      <td>3</td>\n",
       "      <td>[101, 10224, 12714, 72099, 10126, 56879, 10110...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0             La Ant√°rtida es el continente m√°s fr√≠o      7   \n",
       "1  El √°rea de un c√≠rculo se calcula con la f√≥rmul...      9   \n",
       "2  El desierto del Sahara se encuentra en el nort...      7   \n",
       "3  El ensayo es un texto que presenta las ideas d...      2   \n",
       "4  El voleibol se juega en una cancha dividida po...      3   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [101, 10159, 40328, 46532, 11726, 10196, 10125...   \n",
       "1  [101, 10224, 13487, 10104, 10119, 78443, 10126...   \n",
       "2  [101, 10224, 10139, 93548, 10127, 38836, 10126...   \n",
       "3  [101, 10224, 55683, 50253, 10196, 10119, 27888...   \n",
       "4  [101, 10224, 12714, 72099, 10126, 56879, 10110...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = tokenized_dataset['train'].to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2cd9cfc-6a7e-4927-bc8e-91dae74e8b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una m√©trica de evaluaci√≥n\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "def compute_accuracy(predictions_and_labels):\n",
    "    predictions, labels = predictions_and_labels\n",
    "\n",
    "    # El modelo tendr√° outputs logits de la siguiente forma ([[item_n, item_n], [item_m, item_m]]) \n",
    "    # dependiendo del n√∫mero de clases que tenga el problema\n",
    "    # Queramos comparar etiquetas que est√°n en la forma ([0,0,0,1])\n",
    "    if len(predictions.shape) >= 2:\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e75a25d6-591b-4ec5-b5be-440bac265dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Cargando modelo: distilbert/distilbert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Modelo cargado completamente!\n"
     ]
    }
   ],
   "source": [
    "# Seteamos el modelo\n",
    "print(f\"[INFO] Cargando modelo: {MODEL_NAME}\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=MODEL_NAME,\n",
    "    num_labels=10,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "print(f\"[INFO] Modelo cargado completamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed3ded05-9111-45fd-8c44-8b099a3008ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = Path(MODEL_SAVE_DIR_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ffc68ab-39dd-4117-b293-0f9b387a6db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TrainingArguments (these are hyperparameters for our model)\n",
    "# Hyperparameters = settings that we can set as developers\n",
    "# Parameters = settings/weigths our model learns on its own\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = model_save_dir,\n",
    "    learning_rate=0.0001,\n",
    "    per_device_eval_batch_size=32,\n",
    "    per_device_train_batch_size=32,\n",
    "    num_train_epochs=15,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    use_cpu=False,\n",
    "    seed=42,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False,\n",
    "    hub_private_repo=False #Note: this will make our model public by default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2193adc1-6627-493d-85de-ad8262a96aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos instancia de trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_accuracy\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "615b6fcf-46a4-470f-b901-cd549a4f7333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Commencing model training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:40, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.964200</td>\n",
       "      <td>1.515297</td>\n",
       "      <td>0.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.095700</td>\n",
       "      <td>0.955098</td>\n",
       "      <td>0.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.517300</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.234100</td>\n",
       "      <td>0.409817</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.124900</td>\n",
       "      <td>0.335582</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>0.294131</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.048800</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.034100</td>\n",
       "      <td>0.267143</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.025600</td>\n",
       "      <td>0.265655</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.263266</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.260524</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.017400</td>\n",
       "      <td>0.258643</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.256230</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.255346</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>0.254910</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Entrenamos el modelo\n",
    "print(f\"[INFO] Commencing model training...\")\n",
    "results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0ea8a89e-a8f0-48b0-a2b9-f2132f4f73a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model training complete, saving model to a local path: models/learn_hf_spanish_sentence_classification_by_school_subject\n"
     ]
    }
   ],
   "source": [
    "# Guardamos el modelo (to a local directory)\n",
    "print(f\"[INFO] Entrenamiento completado, guardando modelo en siguiente carpeta local: {model_save_dir}\")\n",
    "trainer.save_model(output_dir=model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7184278a-9c8e-4a14-8955-5a1870d38d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Uploading model to Hugging Face Hub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Upload 2 LFS files:   0%|                                 | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "model.safetensors:   0%|                  | 16.4k/541M [00:00<2:53:07, 52.1kB/s]\u001b[A\u001b[A\n",
      "\n",
      "training_args.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.24k/5.24k [00:00<00:00, 5.78kB/s]\u001b[A\u001b[A\n",
      "model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 541M/541M [00:24<00:00, 21.8MB/s]\n",
      "\n",
      "Upload 2 LFS files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:25<00:00, 12.57s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model upload complete, model available at https://huggingface.co/tonicanada/learn_hf_spanish_sentence_classification_by_school_subject/tree/main/\n"
     ]
    }
   ],
   "source": [
    "# Subimos el modelo a HF hub\n",
    "print(f\"[INFO] Subiendo modelo a Hugging Face Hub...\")\n",
    "model_upload_url = trainer.push_to_hub(\n",
    "    commit_message=\"Uploading 'learn_hf_spanish_sentence_classification_by_school_subject'\",\n",
    ")\n",
    "print(f\"[INFO] Modelo subido con √©xito, disponible en {model_upload_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7b30b6d-4cd3-4bc2-a65d-e9b188184345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Realizando evaluaci√≥n en test dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate el modelo con la data test\n",
    "print(f\"[INFO] Realizando evaluaci√≥n en test dataset...\")\n",
    "predictions_all = trainer.predict(tokenized_dataset['test'])\n",
    "predictions_values = predictions_all.predictions\n",
    "predictions_metrics = predictions_all.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "268d7a95-a40b-4244-b863-c571d750d8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] M√©tricas de predicci√≥n en test data:\n",
      "{'test_accuracy': 0.9,\n",
      " 'test_loss': 0.254910409450531,\n",
      " 'test_runtime': 0.0604,\n",
      " 'test_samples_per_second': 827.991,\n",
      " 'test_steps_per_second': 33.12}\n"
     ]
    }
   ],
   "source": [
    "print(f\"[INFO] M√©tricas de predicci√≥n en test data:\")\n",
    "pprint.pprint(predictions_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9df15829-f210-4a3d-b298-50bdae5cf91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'Historia', 'score': 0.9934651255607605}]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probamos el modelo con ejemplos\n",
    "from transformers import pipeline\n",
    "learn_hf_spanish_sentence_classification_by_school_subject = pipeline(task=\"text-classification\",\n",
    "                                    model=model_save_dir,\n",
    "                                    device=torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"),\n",
    "                                    top_k=1,\n",
    "                                    batch_size=32)\n",
    "\n",
    "learn_hf_spanish_sentence_classification_by_school_subject(\"Julio C√©sar fue gobernador de Roma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd503c28-3c4c-4cbe-932e-a8c3b30fa8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'Religi√≥n', 'score': 0.9959214925765991}]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_hf_spanish_sentence_classification_by_school_subject(\"Maoma es un profeta del islam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16118dc0-7ac8-488c-9758-486870e08246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'Frase no relacionada con asignaturas',\n",
       "   'score': 0.6237140893936157}]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_hf_spanish_sentence_classification_by_school_subject(\"Ma√±ana voy a trabajar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8dd0976e-7dc7-4ce5-9733-1c5983af846c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'Educaci√≥n f√≠sica', 'score': 0.8877455592155457}]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_hf_spanish_sentence_classification_by_school_subject(\"Cu√°les son las medidas de la cancha de basket?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eb66b0-c2f0-4972-902c-afd054adec7d",
   "metadata": {},
   "source": [
    "## Creamos una demo a partir de nuestro modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd3ff9d-37f1-4911-94e4-f027f1e2e85b",
   "metadata": {},
   "source": [
    "Necesitamos crear una funci√≥n que entregue el output con la siguiente forma: `{\"label_1\": probability_1, \"label_2\": probability_2,...}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc818a2b-c976-46b2-8e21-0a2585e26b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Educaci√≥n f√≠sica': 0.9700006246566772,\n",
       " 'Frase no relacionada con asignaturas': 0.013027115724980831,\n",
       " 'Geograf√≠a': 0.003420923836529255,\n",
       " 'F√≠sica y qu√≠mica': 0.002429001033306122,\n",
       " 'Religi√≥n': 0.0024216054007411003,\n",
       " 'Historia': 0.0022585378028452396,\n",
       " 'Artes': 0.0019943711813539267,\n",
       " 'Matem√°ticas': 0.001680593704804778,\n",
       " 'Lengua y literatura': 0.0016698952531442046,\n",
       " 'Idiomas extranjeros': 0.0010973839089274406}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "# 1. Create a function to take a string input\n",
    "def spanish_sentence_classification_by_school_subject(text: str) -> Dict[str, float]:\n",
    "    # 2. Setup food not food text classifier\n",
    "    spanish_sentence_classification_by_school_subject_pipeline = pipeline(task=\"text-classification\",\n",
    "                                        model=model_save_dir,\n",
    "                                        batch_size=32,\n",
    "                                        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                                        top_k=None) # top_k=None => return all possible labels\n",
    "\n",
    "    # 3. Get the outputs from our pipeline\n",
    "    outputs = spanish_sentence_classification_by_school_subject_pipeline(text)[0]\n",
    "\n",
    "    # 4. Format output for Gradio\n",
    "    output_dict = {}\n",
    "    for item in outputs:\n",
    "        output_dict[item['label']]=item['score']\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "spanish_sentence_classification_by_school_subject(text=\"El golf es muy entretenido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0bf4d88c-9ad1-4550-8bb6-1e801b46abb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constru√≠mos una demo peque√±a en Gradio para ejecutarla localmente\n",
    "# 1. Import gradio\n",
    "import gradio as gr\n",
    "\n",
    "# 2. Create a gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=spanish_sentence_classification_by_school_subject,\n",
    "    inputs=\"text\",\n",
    "    outputs=gr.Label(num_top_classes=10),\n",
    "    title=\"Detector de asignaturas\",\n",
    "    description=\"Clasificador de texto que detecta la asignatura escolar que tiene referencia con la frase\",\n",
    "    examples=[[\"Matem√°ticas: 5 al cuadrado es 25\"],\n",
    "              [\"Geograf√≠a: Par√≠s es la capital de Francia\"]])\n",
    "\n",
    "# 3. Launch the interface\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e226d97-9bfc-44ca-9693-c7f6c56230d7",
   "metadata": {},
   "source": [
    "### Creamos un directorio para guardar la demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f56a9bff-8d86-44b1-918d-83cd54f59ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Make directory for demos\n",
    "demos_dir = Path(\"./demos\")\n",
    "demos_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create a folder for the food_not_food_text_classifier demo\n",
    "food_not_food_text_classifier_demo_dir = Path(demos_dir, \"spanish_sentence_classification_by_school_subject\")\n",
    "food_not_food_text_classifier_demo_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2bc18585-5a36-4e0f-9e2e-77e0222cefc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./demos/spanish_sentence_classification_by_school_subject/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./demos/spanish_sentence_classification_by_school_subject/app.py\n",
    "# 1. Import the required libraries\n",
    "import torch\n",
    "import gradio as gr\n",
    "\n",
    "from typing import Dict\n",
    "from transformers import pipeline\n",
    "\n",
    "# 2. Define our function to use with our model\n",
    "spanish_sentence_classification_by_school_subject_pipeline = pipeline(task=\"text-classification\",\n",
    "                                    model=\"tonicanada/learn_hf_spanish_sentence_classification_by_school_subject\",\n",
    "                                    top_k=1,\n",
    "                                    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                                    batch_size=32)    \n",
    "\n",
    "def classify_text(text):\n",
    "    # Usa el clasificador\n",
    "    result = spanish_sentence_classification_by_school_subject_pipeline(text)\n",
    "    # Extrae la etiqueta y la puntuaci√≥n (score)\n",
    "    label = result[0][0]['label']\n",
    "    score = result[0][0]['score']\n",
    "    return {label: score}  # Devuelve un diccionario con la etiqueta y la puntuaci√≥n\n",
    "\n",
    "\n",
    "# 3. Create a Gradio interface\n",
    "description = \"\"\"\n",
    "Un clasificador de texto que indica a qu√© asignatura se refiere la frase. \n",
    "\n",
    "Fine-tuned from [DistilBERT](https://huggingface.co/distilbert/distilbert/distilbert-base-multilingual-cased) on a [small dataset of food and not food text](https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions).\n",
    "\"\"\"\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn = classify_text,\n",
    "    inputs = \"text\",\n",
    "    outputs=gr.Label(num_top_classes=10),\n",
    "    title=\"üìöüîç Clasificador de asignaturas\",\n",
    "    description=description,\n",
    "    examples=[[\"Matem√°ticas: 5 al cuadrado es 25\"],\n",
    "                       [\"Geograf√≠a: Par√≠s es la capital de Francia\"]])\n",
    "\n",
    "\n",
    "# 4. Launch the interface\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d028d206-0300-4269-b909-328fe2c6c1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./demos/spanish_sentence_classification_by_school_subject/README.md\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./demos/spanish_sentence_classification_by_school_subject/README.md\n",
    "---\n",
    "title: Clasificador de asignaturas\n",
    "emoji: üìöüîç\n",
    "colorFrom: blue\n",
    "colorTo: yellow\n",
    "sdk: gradio\n",
    "app_file: app.py\n",
    "pinned: false\n",
    "license: apache-2.0\n",
    "---\n",
    "\n",
    "# üìöüîç Clasificador de asignaturas\n",
    "\n",
    "Peque√±a demo que clasifica las frases seg√∫n si se refieren a asignaturas escolares (ejemplo: matem√°ticas, religi√≥n, etc).\n",
    "\n",
    "DistillBERT model fine-tuned on a small synthetic dataset of 250 generated [Frases ejemplo](https://huggingface.co/datasets/tonicanada/learn_hf_spanish_sentence_classification_by_school_subject)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b80238e7-1f63-426a-b460-941ebb87e623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./demos/spanish_sentence_classification_by_school_subject/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./demos/spanish_sentence_classification_by_school_subject/requirements.txt\n",
    "gradio\n",
    "torch\n",
    "transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fd1fa98b-d080-4609-80c8-2a0d96acce1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating repo on Hugging Face Hub with name: learn_hf_spanish_sentence_classification_by_school_subject_demo\n",
      "[INFO] Full hugging face hub repo name: tonicanada/learn_hf_spanish_sentence_classification_by_school_subject_demo\n",
      "[INFO] Uploading ./demos/spanish_sentence_classification_by_school_subject/ to repo tonicanada/learn_hf_spanish_sentence_classification_by_school_subject_demo\n",
      "[INFO] Demo folder successfully uploaded commit url: https://huggingface.co/spaces/tonicanada/learn_hf_spanish_sentence_classification_by_school_subject_demo/tree/main/.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import (\n",
    "    create_repo,\n",
    "    get_full_repo_name,\n",
    "    upload_file,\n",
    "    upload_folder\n",
    ")\n",
    "\n",
    "# Define the parameters we'd like to use for uploading our Space\n",
    "LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD = \"./demos/spanish_sentence_classification_by_school_subject/\"\n",
    "HF_TARGET_SPACE_NAME = \"learn_hf_spanish_sentence_classification_by_school_subject_demo\"\n",
    "HF_REPO_TYPE = \"space\"\n",
    "HF_SPACE_SDK = \"gradio\"\n",
    "\n",
    "# Create a Space repo on Hugging Face Hub\n",
    "print(f\"[INFO] Creating repo on Hugging Face Hub with name: {HF_TARGET_SPACE_NAME}\")\n",
    "create_repo(\n",
    "    repo_id=HF_TARGET_SPACE_NAME,\n",
    "    repo_type = HF_REPO_TYPE,\n",
    "    private=False,\n",
    "    space_sdk=HF_SPACE_SDK,\n",
    "    exist_ok=True\n",
    ")\n",
    "\n",
    "# Get the full repo name (e.g. {username}/{repo_name})\n",
    "hf_full_repo_name = get_full_repo_name(model_id=HF_TARGET_SPACE_NAME)\n",
    "print(f\"[INFO] Full hugging face hub repo name: {hf_full_repo_name}\")\n",
    "\n",
    "# Uploading our demo folder\n",
    "print(f\"[INFO] Uploading {LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD} to repo {hf_full_repo_name}\")\n",
    "folder_upload_url = upload_folder(\n",
    "    repo_id=hf_full_repo_name,\n",
    "    folder_path=LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD,\n",
    "    path_in_repo=\".\",\n",
    "    repo_type=HF_REPO_TYPE,\n",
    "    commit_message=\"Uploading our food not food classifier demo from a notebook!\"\n",
    ")\n",
    "\n",
    "print(f\"[INFO] Demo folder successfully uploaded commit url: {folder_upload_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d0ad38-edea-49ac-9cfb-d9afd1638bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
