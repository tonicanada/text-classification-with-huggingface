{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef6ab608-e5ee-4f5c-bf1a-3bb77c92adcb",
   "metadata": {},
   "source": [
    "# Tutorial proyecto de clasificación de texto mediante Hugging Face (multiclase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feab20f0-13c6-43ba-81eb-c34f77b9cbd9",
   "metadata": {},
   "source": [
    "## ¿Qué vamos a construir?\n",
    "\n",
    "Bienvenido al tutorial del proyecto de clasificación de texto mediante Hugging Face. Este notebook está diseñado para que pueda ser reutilizable para otros casos.\n",
    "En concreto, vamos a construir un modelo, que a partir de una frase en español, va a determinar de qué asignatura se trata, pudiendo ser una de las siguientes:\n",
    "* Religión\n",
    "* Lengua y literatura\n",
    "* Educación física\n",
    "* Artes\n",
    "* Idiomas extranjeros\n",
    "* Historia\n",
    "* Geografía\n",
    "* Física y química\n",
    "* Matemáticas\n",
    "* Frase no relacionada con asignaturas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4db81c8b-0fbe-4272-8a21-cf401af0564a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d022d7c-cdd0-4d07-9d21-fdbc6863e24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acm/Coding/ztm_courses/ztm-huggingface/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Uploading the dataset shards:   0%|                       | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format: 100%|████████| 1/1 [00:00<00:00, 408.56ba/s]\u001b[A\n",
      "Uploading the dataset shards: 100%|███████████████| 1/1 [00:00<00:00,  1.25it/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/tonicanada/learn_hf_spanish_sentence_classification_by_school_subject/commit/daaf3830f889b6ac91ae95714d3464b58c7e3d38', commit_message='Upload dataset', commit_description='', oid='daaf3830f889b6ac91ae95714d3464b58c7e3d38', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/tonicanada/learn_hf_spanish_sentence_classification_by_school_subject', endpoint='https://huggingface.co', repo_type='dataset', repo_id='tonicanada/learn_hf_spanish_sentence_classification_by_school_subject'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pprint\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "# Subimos el dataset a HF datasets\n",
    "DATASET_NAME = \"tonicanada/learn_hf_spanish_sentence_classification_by_school_subject\"\n",
    "df = pd.read_excel(\"./datasets/spanish_sentence_classification_by_school_subject_dataset.xlsx\")\n",
    "dataset = datasets.Dataset.from_pandas(df)\n",
    "repo_id_dataset = DATASET_NAME\n",
    "dataset.push_to_hub(repo_id_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3433c45-9d21-448f-8fb9-9b73a7958fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Definimos las variables para el entrenaiento del modelo y pipeline\n",
    "MODEL_NAME = \"distilbert/distilbert-base-multilingual-cased\"\n",
    "MODEL_SAVE_DIR_NAME=\"models/learn_hf_spanish_sentence_classification_by_school_subject\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c4f5001-05b6-40aa-81ea-2f3464e45701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Descargando dataset desde Hugging Face Hub, nombre: tonicanada/learn_hf_spanish_sentence_classification_by_school_subject\n"
     ]
    }
   ],
   "source": [
    "# 2. Cargamos y preprocesamos el dataset desde Hugging Face Hub\n",
    "print(f\"[INFO] Descargando dataset desde Hugging Face Hub, nombre: {DATASET_NAME}\")\n",
    "dataset = datasets.load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "049de038-533d-4320-8e77-80c6ed47bddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Religión', 1: 'Frase no relacionada con asignaturas', 2: 'Lengua y literatura', 3: 'Educación física', 4: 'Artes', 5: 'Idiomas extranjeros', 6: 'Historia', 7: 'Geografía', 8: 'Física y química', 9: 'Matemáticas'}\n"
     ]
    }
   ],
   "source": [
    "# Creamos una función que permita transformar las labels a id\n",
    "id2label = {idx: label for idx, label in enumerate(dataset['train'].unique('label')[::-1])}\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc4e9468-cd38-44e3-9e92-7802fe9e4b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Religión': 0,\n",
       " 'Frase no relacionada con asignaturas': 1,\n",
       " 'Lengua y literatura': 2,\n",
       " 'Educación física': 3,\n",
       " 'Artes': 4,\n",
       " 'Idiomas extranjeros': 5,\n",
       " 'Historia': 6,\n",
       " 'Geografía': 7,\n",
       " 'Física y química': 8,\n",
       " 'Matemáticas': 9}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {label: id for id, label in id2label.items()}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4261d22-df57-46e2-b3db-83dee80454d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una función para mapear las labels a ID en el dataset\n",
    "def map_labels_to_number(example):\n",
    "    example[\"label\"] = label2id[example[\"label\"]]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a1c0225-f3c3-4af7-8fb8-6e21000c852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train'].map(map_labels_to_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f658c44e-49b6-48b4-850c-d43fe2a78e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos dataset en train/test sets\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5444e153-36ff-4c9e-b92c-9956bc4a8931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La Antártida es el continente más frío</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El área de un círculo se calcula con la fórmul...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El desierto del Sahara se encuentra en el nort...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El ensayo es un texto que presenta las ideas d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>El voleibol se juega en una cancha dividida po...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0             La Antártida es el continente más frío      7\n",
       "1  El área de un círculo se calcula con la fórmul...      9\n",
       "2  El desierto del Sahara se encuentra en el nort...      7\n",
       "3  El ensayo es un texto que presenta las ideas d...      2\n",
       "4  El voleibol se juega en una cancha dividida po...      3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset['train'].to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11b38423-54a5-46d5-b3e9-6b95af4cf303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Tokenizando text para entrenamiento de modelo: distilbert/distilbert-base-multilingual-cased\n"
     ]
    }
   ],
   "source": [
    "# Importamos un tokenizer y lo mapeamos en el dataset\n",
    "print(f\"[INFO] Tokenizando text para entrenamiento de modelo: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=MODEL_NAME,\n",
    "                                          use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa6834d9-fda4-4e9e-9177-951c9a2f185d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '[UNK]', '[SEP]']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hacemos pruebas de conversión de texto a números con el tokenizer\n",
    "tokenizer(\"Ciencia\")\n",
    "tokenizer.convert_ids_to_tokens(tokenizer(\"😆\").input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5666309-a419-4711-b2e4-3124999800bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos función para tokenize la columna \"text\" del dataset\n",
    "def tokenize_text(examples):\n",
    "    \"\"\"\n",
    "    Tokeniza un texto.\n",
    "    \"\"\"\n",
    "    return tokenizer(examples['text'],\n",
    "                    padding=True,\n",
    "                    truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc1b6adc-cd9e-4d7a-8dc7-f9c0a8a66df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 200/200 [00:00<00:00, 23453.49 examples/s]\n",
      "Map: 100%|████████████████████████████| 50/50 [00:00<00:00, 11630.81 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(function=tokenize_text,\n",
    "                                batched=True,\n",
    "                                batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6a77fed-f726-4baa-973b-2ce14a72f2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La Antártida es el continente más frío</td>\n",
       "      <td>7</td>\n",
       "      <td>[101, 10159, 40328, 46532, 11726, 10196, 10125...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El área de un círculo se calcula con la fórmul...</td>\n",
       "      <td>9</td>\n",
       "      <td>[101, 10224, 13487, 10104, 10119, 78443, 10126...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El desierto del Sahara se encuentra en el nort...</td>\n",
       "      <td>7</td>\n",
       "      <td>[101, 10224, 10139, 93548, 10127, 38836, 10126...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El ensayo es un texto que presenta las ideas d...</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 10224, 55683, 50253, 10196, 10119, 27888...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>El voleibol se juega en una cancha dividida po...</td>\n",
       "      <td>3</td>\n",
       "      <td>[101, 10224, 12714, 72099, 10126, 56879, 10110...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0             La Antártida es el continente más frío      7   \n",
       "1  El área de un círculo se calcula con la fórmul...      9   \n",
       "2  El desierto del Sahara se encuentra en el nort...      7   \n",
       "3  El ensayo es un texto que presenta las ideas d...      2   \n",
       "4  El voleibol se juega en una cancha dividida po...      3   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [101, 10159, 40328, 46532, 11726, 10196, 10125...   \n",
       "1  [101, 10224, 13487, 10104, 10119, 78443, 10126...   \n",
       "2  [101, 10224, 10139, 93548, 10127, 38836, 10126...   \n",
       "3  [101, 10224, 55683, 50253, 10196, 10119, 27888...   \n",
       "4  [101, 10224, 12714, 72099, 10126, 56879, 10110...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = tokenized_dataset['train'].to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2cd9cfc-6a7e-4927-bc8e-91dae74e8b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una métrica de evaluación\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "def compute_accuracy(predictions_and_labels):\n",
    "    predictions, labels = predictions_and_labels\n",
    "\n",
    "    # El modelo tendrá outputs logits de la siguiente forma ([[item_n, item_n], [item_m, item_m]]) \n",
    "    # dependiendo del número de clases que tenga el problema\n",
    "    # Queramos comparar etiquetas que están en la forma ([0,0,0,1])\n",
    "    if len(predictions.shape) >= 2:\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e75a25d6-591b-4ec5-b5be-440bac265dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Cargando modelo: distilbert/distilbert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Modelo cargado completamente!\n"
     ]
    }
   ],
   "source": [
    "# Seteamos el modelo\n",
    "print(f\"[INFO] Cargando modelo: {MODEL_NAME}\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=MODEL_NAME,\n",
    "    num_labels=10,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "print(f\"[INFO] Modelo cargado completamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed3ded05-9111-45fd-8c44-8b099a3008ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = Path(MODEL_SAVE_DIR_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ffc68ab-39dd-4117-b293-0f9b387a6db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TrainingArguments (these are hyperparameters for our model)\n",
    "# Hyperparameters = settings that we can set as developers\n",
    "# Parameters = settings/weigths our model learns on its own\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = model_save_dir,\n",
    "    learning_rate=0.0001,\n",
    "    per_device_eval_batch_size=32,\n",
    "    per_device_train_batch_size=32,\n",
    "    num_train_epochs=15,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    use_cpu=False,\n",
    "    seed=42,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False,\n",
    "    hub_private_repo=False #Note: this will make our model public by default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2193adc1-6627-493d-85de-ad8262a96aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos instancia de trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_accuracy\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "615b6fcf-46a4-470f-b901-cd549a4f7333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Commencing model training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:40, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.964200</td>\n",
       "      <td>1.515297</td>\n",
       "      <td>0.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.095700</td>\n",
       "      <td>0.955098</td>\n",
       "      <td>0.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.517300</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.234100</td>\n",
       "      <td>0.409817</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.124900</td>\n",
       "      <td>0.335582</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>0.294131</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.048800</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.034100</td>\n",
       "      <td>0.267143</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.025600</td>\n",
       "      <td>0.265655</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.263266</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.260524</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.017400</td>\n",
       "      <td>0.258643</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.256230</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.255346</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>0.254910</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Entrenamos el modelo\n",
    "print(f\"[INFO] Commencing model training...\")\n",
    "results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0ea8a89e-a8f0-48b0-a2b9-f2132f4f73a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model training complete, saving model to a local path: models/learn_hf_spanish_sentence_classification_by_school_subject\n"
     ]
    }
   ],
   "source": [
    "# Guardamos el modelo (to a local directory)\n",
    "print(f\"[INFO] Entrenamiento completado, guardando modelo en siguiente carpeta local: {model_save_dir}\")\n",
    "trainer.save_model(output_dir=model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7184278a-9c8e-4a14-8955-5a1870d38d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Uploading model to Hugging Face Hub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Upload 2 LFS files:   0%|                                 | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "model.safetensors:   0%|                  | 16.4k/541M [00:00<2:53:07, 52.1kB/s]\u001b[A\u001b[A\n",
      "\n",
      "training_args.bin: 100%|███████████████████| 5.24k/5.24k [00:00<00:00, 5.78kB/s]\u001b[A\u001b[A\n",
      "model.safetensors: 100%|█████████████████████| 541M/541M [00:24<00:00, 21.8MB/s]\n",
      "\n",
      "Upload 2 LFS files: 100%|█████████████████████████| 2/2 [00:25<00:00, 12.57s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model upload complete, model available at https://huggingface.co/tonicanada/learn_hf_spanish_sentence_classification_by_school_subject/tree/main/\n"
     ]
    }
   ],
   "source": [
    "# Subimos el modelo a HF hub\n",
    "print(f\"[INFO] Subiendo modelo a Hugging Face Hub...\")\n",
    "model_upload_url = trainer.push_to_hub(\n",
    "    commit_message=\"Uploading 'learn_hf_spanish_sentence_classification_by_school_subject'\",\n",
    ")\n",
    "print(f\"[INFO] Modelo subido con éxito, disponible en {model_upload_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7b30b6d-4cd3-4bc2-a65d-e9b188184345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Realizando evaluación en test dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate el modelo con la data test\n",
    "print(f\"[INFO] Realizando evaluación en test dataset...\")\n",
    "predictions_all = trainer.predict(tokenized_dataset['test'])\n",
    "predictions_values = predictions_all.predictions\n",
    "predictions_metrics = predictions_all.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "268d7a95-a40b-4244-b863-c571d750d8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Métricas de predicción en test data:\n",
      "{'test_accuracy': 0.9,\n",
      " 'test_loss': 0.254910409450531,\n",
      " 'test_runtime': 0.0604,\n",
      " 'test_samples_per_second': 827.991,\n",
      " 'test_steps_per_second': 33.12}\n"
     ]
    }
   ],
   "source": [
    "print(f\"[INFO] Métricas de predicción en test data:\")\n",
    "pprint.pprint(predictions_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9df15829-f210-4a3d-b298-50bdae5cf91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'Historia', 'score': 0.9934651255607605}]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probamos el modelo con ejemplos\n",
    "from transformers import pipeline\n",
    "learn_hf_spanish_sentence_classification_by_school_subject = pipeline(task=\"text-classification\",\n",
    "                                    model=model_save_dir,\n",
    "                                    device=torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"),\n",
    "                                    top_k=1,\n",
    "                                    batch_size=32)\n",
    "\n",
    "learn_hf_spanish_sentence_classification_by_school_subject(\"Julio César fue gobernador de Roma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd503c28-3c4c-4cbe-932e-a8c3b30fa8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'Religión', 'score': 0.9959214925765991}]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_hf_spanish_sentence_classification_by_school_subject(\"Maoma es un profeta del islam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16118dc0-7ac8-488c-9758-486870e08246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'Frase no relacionada con asignaturas',\n",
       "   'score': 0.6237140893936157}]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_hf_spanish_sentence_classification_by_school_subject(\"Mañana voy a trabajar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8dd0976e-7dc7-4ce5-9733-1c5983af846c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'Educación física', 'score': 0.8877455592155457}]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_hf_spanish_sentence_classification_by_school_subject(\"Cuáles son las medidas de la cancha de basket?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eb66b0-c2f0-4972-902c-afd054adec7d",
   "metadata": {},
   "source": [
    "## Creamos una demo a partir de nuestro modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd3ff9d-37f1-4911-94e4-f027f1e2e85b",
   "metadata": {},
   "source": [
    "Necesitamos crear una función que entregue el output con la siguiente forma: `{\"label_1\": probability_1, \"label_2\": probability_2,...}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc818a2b-c976-46b2-8e21-0a2585e26b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Educación física': 0.9700006246566772,\n",
       " 'Frase no relacionada con asignaturas': 0.013027115724980831,\n",
       " 'Geografía': 0.003420923836529255,\n",
       " 'Física y química': 0.002429001033306122,\n",
       " 'Religión': 0.0024216054007411003,\n",
       " 'Historia': 0.0022585378028452396,\n",
       " 'Artes': 0.0019943711813539267,\n",
       " 'Matemáticas': 0.001680593704804778,\n",
       " 'Lengua y literatura': 0.0016698952531442046,\n",
       " 'Idiomas extranjeros': 0.0010973839089274406}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "# 1. Create a function to take a string input\n",
    "def spanish_sentence_classification_by_school_subject(text: str) -> Dict[str, float]:\n",
    "    # 2. Setup food not food text classifier\n",
    "    spanish_sentence_classification_by_school_subject_pipeline = pipeline(task=\"text-classification\",\n",
    "                                        model=model_save_dir,\n",
    "                                        batch_size=32,\n",
    "                                        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                                        top_k=None) # top_k=None => return all possible labels\n",
    "\n",
    "    # 3. Get the outputs from our pipeline\n",
    "    outputs = spanish_sentence_classification_by_school_subject_pipeline(text)[0]\n",
    "\n",
    "    # 4. Format output for Gradio\n",
    "    output_dict = {}\n",
    "    for item in outputs:\n",
    "        output_dict[item['label']]=item['score']\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "spanish_sentence_classification_by_school_subject(text=\"El golf es muy entretenido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0bf4d88c-9ad1-4550-8bb6-1e801b46abb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construímos una demo pequeña en Gradio para ejecutarla localmente\n",
    "# 1. Import gradio\n",
    "import gradio as gr\n",
    "\n",
    "# 2. Create a gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=spanish_sentence_classification_by_school_subject,\n",
    "    inputs=\"text\",\n",
    "    outputs=gr.Label(num_top_classes=10),\n",
    "    title=\"Detector de asignaturas\",\n",
    "    description=\"Clasificador de texto que detecta la asignatura escolar que tiene referencia con la frase\",\n",
    "    examples=[[\"Matemáticas: 5 al cuadrado es 25\"],\n",
    "              [\"Geografía: París es la capital de Francia\"]])\n",
    "\n",
    "# 3. Launch the interface\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e226d97-9bfc-44ca-9693-c7f6c56230d7",
   "metadata": {},
   "source": [
    "### Creamos un directorio para guardar la demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f56a9bff-8d86-44b1-918d-83cd54f59ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Make directory for demos\n",
    "demos_dir = Path(\"./demos\")\n",
    "demos_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create a folder for the food_not_food_text_classifier demo\n",
    "food_not_food_text_classifier_demo_dir = Path(demos_dir, \"spanish_sentence_classification_by_school_subject\")\n",
    "food_not_food_text_classifier_demo_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2bc18585-5a36-4e0f-9e2e-77e0222cefc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./demos/spanish_sentence_classification_by_school_subject/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./demos/spanish_sentence_classification_by_school_subject/app.py\n",
    "# 1. Import the required libraries\n",
    "import torch\n",
    "import gradio as gr\n",
    "\n",
    "from typing import Dict\n",
    "from transformers import pipeline\n",
    "\n",
    "# 2. Define our function to use with our model\n",
    "spanish_sentence_classification_by_school_subject_pipeline = pipeline(task=\"text-classification\",\n",
    "                                    model=\"tonicanada/learn_hf_spanish_sentence_classification_by_school_subject\",\n",
    "                                    top_k=1,\n",
    "                                    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                                    batch_size=32)    \n",
    "\n",
    "def classify_text(text):\n",
    "    # Usa el clasificador\n",
    "    result = spanish_sentence_classification_by_school_subject_pipeline(text)\n",
    "    # Extrae la etiqueta y la puntuación (score)\n",
    "    label = result[0][0]['label']\n",
    "    score = result[0][0]['score']\n",
    "    return {label: score}  # Devuelve un diccionario con la etiqueta y la puntuación\n",
    "\n",
    "\n",
    "# 3. Create a Gradio interface\n",
    "description = \"\"\"\n",
    "Un clasificador de texto que indica a qué asignatura se refiere la frase. \n",
    "\n",
    "Fine-tuned from [DistilBERT](https://huggingface.co/distilbert/distilbert/distilbert-base-multilingual-cased) on a [small dataset of food and not food text](https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions).\n",
    "\"\"\"\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn = classify_text,\n",
    "    inputs = \"text\",\n",
    "    outputs=gr.Label(num_top_classes=10),\n",
    "    title=\"📚🔍 Clasificador de asignaturas\",\n",
    "    description=description,\n",
    "    examples=[[\"Matemáticas: 5 al cuadrado es 25\"],\n",
    "                       [\"Geografía: París es la capital de Francia\"]])\n",
    "\n",
    "\n",
    "# 4. Launch the interface\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d028d206-0300-4269-b909-328fe2c6c1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./demos/spanish_sentence_classification_by_school_subject/README.md\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./demos/spanish_sentence_classification_by_school_subject/README.md\n",
    "---\n",
    "title: Clasificador de asignaturas\n",
    "emoji: 📚🔍\n",
    "colorFrom: blue\n",
    "colorTo: yellow\n",
    "sdk: gradio\n",
    "app_file: app.py\n",
    "pinned: false\n",
    "license: apache-2.0\n",
    "---\n",
    "\n",
    "# 📚🔍 Clasificador de asignaturas\n",
    "\n",
    "Pequeña demo que clasifica las frases según si se refieren a asignaturas escolares (ejemplo: matemáticas, religión, etc).\n",
    "\n",
    "DistillBERT model fine-tuned on a small synthetic dataset of 250 generated [Frases ejemplo](https://huggingface.co/datasets/tonicanada/learn_hf_spanish_sentence_classification_by_school_subject)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b80238e7-1f63-426a-b460-941ebb87e623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./demos/spanish_sentence_classification_by_school_subject/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./demos/spanish_sentence_classification_by_school_subject/requirements.txt\n",
    "gradio\n",
    "torch\n",
    "transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fd1fa98b-d080-4609-80c8-2a0d96acce1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating repo on Hugging Face Hub with name: learn_hf_spanish_sentence_classification_by_school_subject_demo\n",
      "[INFO] Full hugging face hub repo name: tonicanada/learn_hf_spanish_sentence_classification_by_school_subject_demo\n",
      "[INFO] Uploading ./demos/spanish_sentence_classification_by_school_subject/ to repo tonicanada/learn_hf_spanish_sentence_classification_by_school_subject_demo\n",
      "[INFO] Demo folder successfully uploaded commit url: https://huggingface.co/spaces/tonicanada/learn_hf_spanish_sentence_classification_by_school_subject_demo/tree/main/.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import (\n",
    "    create_repo,\n",
    "    get_full_repo_name,\n",
    "    upload_file,\n",
    "    upload_folder\n",
    ")\n",
    "\n",
    "# Define the parameters we'd like to use for uploading our Space\n",
    "LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD = \"./demos/spanish_sentence_classification_by_school_subject/\"\n",
    "HF_TARGET_SPACE_NAME = \"learn_hf_spanish_sentence_classification_by_school_subject_demo\"\n",
    "HF_REPO_TYPE = \"space\"\n",
    "HF_SPACE_SDK = \"gradio\"\n",
    "\n",
    "# Create a Space repo on Hugging Face Hub\n",
    "print(f\"[INFO] Creating repo on Hugging Face Hub with name: {HF_TARGET_SPACE_NAME}\")\n",
    "create_repo(\n",
    "    repo_id=HF_TARGET_SPACE_NAME,\n",
    "    repo_type = HF_REPO_TYPE,\n",
    "    private=False,\n",
    "    space_sdk=HF_SPACE_SDK,\n",
    "    exist_ok=True\n",
    ")\n",
    "\n",
    "# Get the full repo name (e.g. {username}/{repo_name})\n",
    "hf_full_repo_name = get_full_repo_name(model_id=HF_TARGET_SPACE_NAME)\n",
    "print(f\"[INFO] Full hugging face hub repo name: {hf_full_repo_name}\")\n",
    "\n",
    "# Uploading our demo folder\n",
    "print(f\"[INFO] Uploading {LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD} to repo {hf_full_repo_name}\")\n",
    "folder_upload_url = upload_folder(\n",
    "    repo_id=hf_full_repo_name,\n",
    "    folder_path=LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD,\n",
    "    path_in_repo=\".\",\n",
    "    repo_type=HF_REPO_TYPE,\n",
    "    commit_message=\"Uploading our food not food classifier demo from a notebook!\"\n",
    ")\n",
    "\n",
    "print(f\"[INFO] Demo folder successfully uploaded commit url: {folder_upload_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d0ad38-edea-49ac-9cfb-d9afd1638bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
